{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73b79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac5c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all asume that values start from row 2 (i.e., 1st row is header)\n",
    "def filter_deltaAnnot(data, tol):\n",
    "    return np.where(np.abs(data.iloc[2:,2])>tol)[0]+2\n",
    "\n",
    "# inplace transform\n",
    "def fill_gap_nan_(data, value):\n",
    "    data.iloc[2:,88:].fillna(value, inplace=True)\n",
    "\n",
    "# inplace transform\n",
    "def log2_transform_(data):\n",
    "    for i in range(2,data.shape[0]):\n",
    "        for j in range(8,48):\n",
    "            data.iloc[i,j]=np.log2(data.iloc[i,j])\n",
    "    \n",
    "def get_VAL_duplicates(data, tol):\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(2,data.shape[0]-1):\n",
    "        a = np.asarray(data.iloc[i,8:48], dtype=float)\n",
    "        for j in range(i+1,data.shape[0]):\n",
    "            b = np.asarray(data.iloc[j,8:48], dtype=float)\n",
    "            if(np.sum(a==b)/len(a) >= tol):\n",
    "                indices.append((i,j))\n",
    "    result=np.zeros((len(indices),2))\n",
    "    for i in range(result.shape[0]):\n",
    "        result[i,0]=indices[i][0]\n",
    "        result[i,1]=indices[i][1]\n",
    "    return result\n",
    "\n",
    "@jit(nopython=True)\n",
    "def duplicates(data, tol):\n",
    "    n = data.shape[0]\n",
    "    result = np.zeros((n+2,))\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        a = data[i,:]\n",
    "        for j in range(i+1,n):\n",
    "            b = data[j,:]\n",
    "            if(np.sum(a==b)/len(a) >= tol):\n",
    "                result[i+2]=1\n",
    "                result[j+2]=1\n",
    "                #continue\n",
    "                \n",
    "    return result, np.array([i for i in np.arange(n) if result[i]])\n",
    "\n",
    "def get_VAL_duplicates_fast(data, tol):\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(2,data.shape[0]-1):\n",
    "        a = np.asarray(data.iloc[i,8:48], dtype=float)\n",
    "        for j in range(i+1,data.shape[0]):\n",
    "            b = np.asarray(data.iloc[j,8:48], dtype=float)\n",
    "            if(np.sum(a==b)/len(a) >= tol):\n",
    "                indices.append((i,j))\n",
    "                continue\n",
    "    result=np.zeros((len(indices),2))\n",
    "    for i in range(result.shape[0]):\n",
    "        result[i,0]=indices[i][0]\n",
    "        result[i,1]=indices[i][1]\n",
    "    return np.asarray(result,dtype=int).flatten()\n",
    "\n",
    "def ind2bool(inds,n):\n",
    "    lam = lambda i: i in inds\n",
    "    result = np.array([lam(i) for i in range(n)])\n",
    "    return result\n",
    "    \n",
    "\n",
    "def check_reliability(data, tol=0.8, columns= np.array(range(88,128)),acceptable=np.array([0,64,128])):\n",
    "    tol=0.8\n",
    "    n=columns.shape[0]\n",
    "    accept= np.array([0,64,128])\n",
    "    keep=[]\n",
    "    for i in range(2,data.shape[0]):\n",
    "        a=np.asarray(data.iloc[i,columns],dtype=int)\n",
    "        s=0\n",
    "        for elem in a:\n",
    "            s+=int(elem in accept)\n",
    "        if(s>=tol*n):\n",
    "            keep.append(i)\n",
    "    return np.array(keep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270531e0",
   "metadata": {},
   "source": [
    "### Manually first prune QCs out of every sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f9e29",
   "metadata": {},
   "source": [
    "This allows you to slice 8:48 sample ID's, and then use those to get matching gp_status and gp_method columns. Also check that the order is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8499815",
   "metadata": {},
   "source": [
    "### List of all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5590d44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cellshilicnegGFstatus.xlsx',\n",
       " 'cellshilicposGFstatus.xlsx',\n",
       " 'cellslipidnegGFstatus.xlsx',\n",
       " 'cellslipidposGFstatus.xlsx',\n",
       " 'cellsRPnegGFstatus.xlsx',\n",
       " 'cellsRPposGFstatus.xlsx',\n",
       " 'mediumhilicnegGFstatus.xlsx',\n",
       " 'mediumhilicposGFstatus.xlsx',\n",
       " 'mediumRPnegGFstatus.xlsx',\n",
       " 'mediumRPposGFstatus.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../PDproj/celldata/datawithGFstatus/original/'\n",
    "path_out = '../PDproj/celldata/datawithGFstatus/clean/v2/'\n",
    "fname = 'cellshilicnegGFstatus.xlsx'\n",
    "filenames = [line.rstrip() for line in open(path +'filenames.txt')]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ca401",
   "metadata": {},
   "source": [
    "### all files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81db2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing cellshilicnegGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing cellshilicposGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing cellslipidnegGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing cellslipidposGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing cellsRPnegGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing cellsRPposGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing mediumhilicnegGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing mediumhilicposGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing mediumRPnegGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n",
      "Now processing mediumRPposGFstatus.xlsx\n",
      "Keep 128 columns: True\n",
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n"
     ]
    }
   ],
   "source": [
    "for fnum in range(10):\n",
    "    print(\"Now processing {}\".format(filenames[fnum]))\n",
    "    data = pd.read_excel(path + filenames[fnum])\n",
    "    colnames = data.columns\n",
    "    \n",
    "    th=1000\n",
    "    #drop duplicates with high deltamass\n",
    "    #dup=get_VAL_duplicates_fast(data,tol=1)\n",
    "    #to_drop=np.array(list(set( np.unique(dup.flatten())) & set(filter_deltaAnnot(data,th))), dtype=int)\n",
    "    \n",
    "    # drop all with high deltamass\n",
    "    to_drop=filter_deltaAnnot(data,th)\n",
    "    data=data.drop(filter_deltaAnnot(data,th),axis=0)\n",
    "    \n",
    "    IDs=[]\n",
    "    for s in data.iloc[1,8:]:\n",
    "        #print(s)\n",
    "        IDs.append(int(s.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    IDs=np.array(IDs)\n",
    "    area_id = IDs[:40]\n",
    "    GAP_id = IDs[40:]\n",
    "    bools = np.asarray([GAP_id[i] in area_id for i in range(len(GAP_id))], dtype=int)\n",
    "    bools=np.hstack([np.ones((48,)),bools])\n",
    "    keep_ind = np.array([i for i in range(len(bools)) if bools[i]])\n",
    "    print(\"Keep 128 columns: {}\".format(keep_ind.shape[0]==128))\n",
    "    \n",
    "    data = data.iloc[:,keep_ind]\n",
    "    print('Number of groups: {} (40 in each area, gp method, gp status)'.format((data.shape[1]-8)/40))\n",
    "\n",
    "    IDs=[]\n",
    "    for s in data.iloc[1,8:]:\n",
    "        #print(s)\n",
    "        IDs.append(int(s.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    IDs=np.array(IDs)\n",
    "    area_id = IDs[:40]\n",
    "    GAP_id = IDs[40:]\n",
    "\n",
    "    print(\"Order matches: {}\".format(np.alltrue(area_id==GAP_id[:40]) and np.alltrue(area_id==GAP_id[40:])))\n",
    "    print(\"Number of unique ID's: {}\".format(np.unique(IDs).shape[0]))\n",
    "    \n",
    "    \n",
    "    log2_transform_(data)\n",
    "    fill_gap_nan_(data,0)\n",
    "    \n",
    "    new_colnames=['aSYN' for i in range(10)] + ['comb.' for i in range(10)] + ['INFg' for i in range(10)] + ['UT' for i in range(10)]\n",
    "    new_colnames=['' for i in range(8)] + new_colnames + ['' for i in range(80)]\n",
    "    data.columns=np.array(new_colnames)\n",
    "    \n",
    "    fname_out = path_out + filenames[fnum].split('.')[0] + '.csv'\n",
    "    data.to_csv(fname_out, header=True, index=False, sep=';')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ed046",
   "metadata": {},
   "source": [
    "### Select a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd8ed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cellshilicnegGFstatus.xlsx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnum=0\n",
    "data = pd.read_excel(path + filenames[fnum])\n",
    "colnames = data.columns\n",
    "filenames[fnum]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b8930",
   "metadata": {},
   "source": [
    "### Drop rows with delta Annot over a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c42519f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,i=duplicates(data.iloc[2:,8:48].to_numpy(dtype=float),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d29139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup=get_VAL_duplicates(data,tol=1)\n",
    "to_drop=np.array(list(set( np.unique(dup.flatten())) & set(filter_deltaAnnot(data,1000))), dtype=int)\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9191362c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81, 117])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop=filter_deltaAnnot(data,1000)\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65015563",
   "metadata": {},
   "outputs": [],
   "source": [
    "th=1000\n",
    "data=data.drop(to_drop,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f870dd",
   "metadata": {},
   "source": [
    "### Align the id's and get indices to prune out excess columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cda33055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep 128 columns: True\n"
     ]
    }
   ],
   "source": [
    "IDs=[]\n",
    "for s in data.iloc[1,8:]:\n",
    "    #print(s)\n",
    "    IDs.append(int(s.split('_')[-1].split('.')[0]))\n",
    "\n",
    "IDs=np.array(IDs)\n",
    "area_id = IDs[:40]\n",
    "GAP_id = IDs[40:]\n",
    "bools = np.asarray([GAP_id[i] in area_id for i in range(len(GAP_id))], dtype=int)\n",
    "bools=np.hstack([np.ones((48,)),bools])\n",
    "keep_ind = np.array([i for i in range(len(bools)) if bools[i]])\n",
    "print(\"Keep 128 columns: {}\".format(keep_ind.shape[0]==128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de738a",
   "metadata": {},
   "source": [
    "### Remove the excess columns (id's) and check that the result is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e8f1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 3.0 (40 in each area, gp method, gp status)\n",
      "Order matches: True\n",
      "Number of unique ID's: 40\n"
     ]
    }
   ],
   "source": [
    "data = data.iloc[:,keep_ind]\n",
    "print('Number of groups: {} (40 in each area, gp method, gp status)'.format((data.shape[1]-8)/40))\n",
    "\n",
    "IDs=[]\n",
    "for s in data.iloc[1,8:]:\n",
    "    #print(s)\n",
    "    IDs.append(int(s.split('_')[-1].split('.')[0]))\n",
    "\n",
    "IDs=np.array(IDs)\n",
    "area_id = IDs[:40]\n",
    "GAP_id = IDs[40:]\n",
    "\n",
    "print(\"Order matches: {}\".format(np.alltrue(area_id==GAP_id[:40]) and np.alltrue(area_id==GAP_id[40:])))\n",
    "print(\"Number of unique ID's: {}\".format(np.unique(IDs).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c8ee1",
   "metadata": {},
   "source": [
    "### Log transform (inplace) and fill nan's (inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70587c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_transform_(data)\n",
    "fill_gap_nan_(data,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb718c",
   "metadata": {},
   "source": [
    "### Reliability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0e3fa927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep_rows=check_reliability(data,columns=np.array(range(88,98)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c55f1e",
   "metadata": {},
   "source": [
    "### Find value duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a0c50ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate_ind=get_VAL_duplicates(data,1)\n",
    "#duplicate_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "886a9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.iloc[[13,49]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b5af2",
   "metadata": {},
   "source": [
    "**Correct the columnnames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b68488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colnames=['aSYN' for i in range(10)] + ['comb.' for i in range(10)] + ['INFg' for i in range(10)] + ['UT' for i in range(10)]\n",
    "new_colnames=['' for i in range(8)] + new_colnames + ['' for i in range(80)]\n",
    "data.columns=np.array(new_colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0065c6",
   "metadata": {},
   "source": [
    "**And save to file named:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b3d4a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../PDproj/celldata/datawithGFstatus/clean/v2/cellshilicnegGFstatus.csv'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname_out = path_out + filenames[fnum].split('.')[0] + '.csv'\n",
    "fname_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578d093",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ada4a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(fname_out, header=True, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f993c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cellshilicnegGFstatus.csv',\n",
       " 'cellshilicposGFstatus.csv',\n",
       " 'cellslipidnegGFstatus.csv',\n",
       " 'cellslipidposGFstatus.csv',\n",
       " 'cellsRPnegGFstatus.csv',\n",
       " 'cellsRPposGFstatus.csv',\n",
       " 'mediumhilicnegGFstatus.csv',\n",
       " 'mediumhilicposGFstatus.csv',\n",
       " 'mediumRPnegGFstatus.csv',\n",
       " 'mediumRPposGFstatus.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../PDproj/celldata/datawithGFstatus/clean/'\n",
    "filenames = [line.rstrip() for line in open(path +'filenames.txt')]\n",
    "filenames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
